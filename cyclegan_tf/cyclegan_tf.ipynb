{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imsave\n",
    "import os\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import time\n",
    "import random\n",
    "import sys\n",
    "from model import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "to_train = True\n",
    "to_test = False\n",
    "to_restore = False\n",
    "save_model = False\n",
    "save_train_image = True\n",
    "save_scalar = False\n",
    "save_graph = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.img_height = 256\n",
    "        self.img_width = 256\n",
    "        self.img_layer = 3\n",
    "        self.batch_size = 1\n",
    "        self.img_size = self.img_height * self.img_width\n",
    "        \n",
    "        self.pool_size = 50\n",
    "        self.learning_rate = 0.0002\n",
    "        self.mylambda = 10\n",
    "        self.max_epoch = 20\n",
    "        self.max_images = 100\n",
    "        \n",
    "        self.check_dir_write = \"./output/write/\"\n",
    "        self.check_dir = \"./output/checkpoints/\"\n",
    "        self.train_path_a = \"datasets/apple2orange/trainA/*\"\n",
    "        self.train_path_b = \"datasets/apple2orange/trainB/*\"\n",
    "\n",
    "\n",
    "    def input_setup(self):\n",
    "\n",
    "        '''\n",
    "        This function basically setup variables for taking image input.\n",
    "        filenames_A/filenames_B -> takes the list of all training images\n",
    "        self.image_A/self.image_B -> Input image with each values ranging from [-1,1]\n",
    "        '''\n",
    "\n",
    "        filenames_A = tf.train.match_filenames_once(self.train_path_a)\n",
    "        self.queue_length_A = tf.size(filenames_A)  #图片A的数目\n",
    "\n",
    "        filenames_B = tf.train.match_filenames_once(self.train_path_b)\n",
    "        self.queue_length_B = tf.size(filenames_B)  #图片B的数目\n",
    "\n",
    "        filename_queue_A = tf.train.string_input_producer(filenames_A)\n",
    "        filename_queue_B = tf.train.string_input_producer(filenames_B)\n",
    "\n",
    "        image_reader = tf.WholeFileReader()\n",
    "        _, image_file_A = image_reader.read(filename_queue_A)\n",
    "        _, image_file_B = image_reader.read(filename_queue_B)\n",
    "        \n",
    "        #rgb/127.5-1映射到[-1,1]\n",
    "        # w=256,h=256\n",
    "        self.image_A = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_A, channels=3),[256,256]),127.5),1) \n",
    "\n",
    "        self.image_B = tf.subtract(tf.div(tf.image.resize_images(tf.image.decode_jpeg(image_file_B, channels=3),[256,256]),127.5),1)    \n",
    "\n",
    "\n",
    "\n",
    "    def input_read(self, sess):\n",
    "\n",
    "        '''\n",
    "        It reads the input into from the image folder.\n",
    "        self.fake_images_A/self.fake_images_B -> List of generated images used for calculation of loss function of Discriminator\n",
    "        self.A_input/self.B_input -> Stores all the training images in python list\n",
    "        '''\n",
    "\n",
    "        # Loading images into the tensors\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "\n",
    "        num_files_A = sess.run(self.queue_length_A)\n",
    "        num_files_B = sess.run(self.queue_length_B)\n",
    "\n",
    "        self.fake_images_A = np.zeros((self.pool_size,1,self.img_height, self.img_width, self.img_layer))\n",
    "        self.fake_images_B = np.zeros((self.pool_size,1,self.img_height, self.img_width, self.img_layer))\n",
    "\n",
    "\n",
    "        self.A_input = np.zeros((self.max_images, self.batch_size, self.img_height, self.img_width, self.img_layer))\n",
    "        self.B_input = np.zeros((self.max_images, self.batch_size, self.img_height, self.img_width, self.img_layer))\n",
    "\n",
    "        for i in range(self.max_images):\n",
    "            image_tensor = sess.run(self.image_A)\n",
    "            if image_tensor.size == self.img_size*self.batch_size*self.img_layer :\n",
    "                self.A_input[i] = image_tensor.reshape((self.batch_size,self.img_height, self.img_width, self.img_layer))\n",
    "\n",
    "        for i in range(self.max_images):\n",
    "            image_tensor = sess.run(self.image_B)\n",
    "            if image_tensor.size == self.img_size*self.batch_size*self.img_layer :\n",
    "                self.B_input[i] = image_tensor.reshape((self.batch_size,self.img_height, self.img_width, self.img_layer))\n",
    "\n",
    "\n",
    "        coord.request_stop() # coord发出所有线程终止信号 \n",
    "        coord.join(threads) # 开启的线程加入主线程，等待threads结束  \n",
    "\n",
    "\n",
    "\n",
    "    def model_setup(self):\n",
    "\n",
    "        ''' This function sets up the model to train\n",
    "        self.input_A/self.input_B -> Set of training images.\n",
    "        self.fake_A/self.fake_B -> Generated images by corresponding generator of input_A and input_B\n",
    "        self.lr -> Learning rate variable\n",
    "        self.cyc_A/ self.cyc_B -> Images generated after feeding self.fake_A/self.fake_B to corresponding generator. \n",
    "        This is use to calcualte cyclic loss\n",
    "        '''\n",
    "\n",
    "        self.input_A = tf.placeholder(tf.float32, [self.batch_size, self.img_width, self.img_height, self.img_layer], name=\"input_A\")\n",
    "        self.input_B = tf.placeholder(tf.float32, [self.batch_size, self.img_width, self.img_height, self.img_layer], name=\"input_B\")\n",
    "\n",
    "        self.fake_pool_A = tf.placeholder(tf.float32, [None, self.img_width, self.img_height, self.img_layer], name=\"fake_pool_A\")\n",
    "        self.fake_pool_B = tf.placeholder(tf.float32, [None, self.img_width, self.img_height, self.img_layer], name=\"fake_pool_B\")\n",
    "\n",
    "        self.global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "        self.num_fake_inputs = 0\n",
    "\n",
    "        self.lr = tf.placeholder(tf.float32, shape=[], name=\"lr\")\n",
    "\n",
    "        with tf.variable_scope(\"Model\") as scope:\n",
    "            self.fake_B = build_generator_resnet_9blocks(self.input_A, name=\"g_A\")  #fake_B是由A生成的B\n",
    "            self.fake_A = build_generator_resnet_9blocks(self.input_B, name=\"g_B\")  #fake_A是由B生成的A\n",
    "            self.rec_A = build_gen_discriminator(self.input_A, \"d_A\")\n",
    "            self.rec_B = build_gen_discriminator(self.input_B, \"d_B\")\n",
    "\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            self.fake_rec_A = build_gen_discriminator(self.fake_A, \"d_A\")\n",
    "            self.fake_rec_B = build_gen_discriminator(self.fake_B, \"d_B\")\n",
    "            self.cyc_A = build_generator_resnet_9blocks(self.fake_B, \"g_B\")     #cyc_A是由A生成的B再生成的A\n",
    "            self.cyc_B = build_generator_resnet_9blocks(self.fake_A, \"g_A\")     #cyc_B是由B生成的A再生成的B\n",
    "\n",
    "            scope.reuse_variables()\n",
    "\n",
    "            self.fake_pool_rec_A = build_gen_discriminator(self.fake_pool_A, \"d_A\")\n",
    "            self.fake_pool_rec_B = build_gen_discriminator(self.fake_pool_B, \"d_B\")\n",
    "\n",
    "            \n",
    "    def loss_cal(self):\n",
    "\n",
    "        ''' In this function we are defining the variables for loss calcultions and traning model\n",
    "        d_loss_A/d_loss_B -> loss for discriminator A/B\n",
    "        g_loss_A/g_loss_B -> loss for generator A/B\n",
    "        *_trainer -> Variaous trainer for above loss functions\n",
    "        *_summ -> Summary variables for above loss functions'''\n",
    "\n",
    "        cyc_loss = tf.reduce_mean(tf.abs(self.input_A-self.cyc_A)) + tf.reduce_mean(tf.abs(self.input_B-self.cyc_B))\n",
    "\n",
    "        disc_loss_A = tf.reduce_mean(tf.squared_difference(self.fake_rec_A,1))  #fake_rec_A is G_B(input A)\n",
    "        disc_loss_B = tf.reduce_mean(tf.squared_difference(self.fake_rec_B,1))  #fake_rec_B is G_A(input B)\n",
    "\n",
    "        g_loss_A = cyc_loss*self.mylambda + disc_loss_B\n",
    "        g_loss_B = cyc_loss*self.mylambda + disc_loss_A\n",
    "\n",
    "        d_loss_A = (tf.reduce_mean(tf.square(self.fake_pool_rec_A)) + tf.reduce_mean(tf.squared_difference(self.rec_A,1)))/2.0\n",
    "        d_loss_B = (tf.reduce_mean(tf.square(self.fake_pool_rec_B)) + tf.reduce_mean(tf.squared_difference(self.rec_B,1)))/2.0\n",
    "\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(self.lr, beta1=0.5)\n",
    "\n",
    "        self.model_vars = tf.trainable_variables()\n",
    "\n",
    "        d_A_vars = [var for var in self.model_vars if 'd_A' in var.name]\n",
    "        g_A_vars = [var for var in self.model_vars if 'g_A' in var.name]\n",
    "        d_B_vars = [var for var in self.model_vars if 'd_B' in var.name]\n",
    "        g_B_vars = [var for var in self.model_vars if 'g_B' in var.name]\n",
    "\n",
    "        self.d_A_trainer = optimizer.minimize(d_loss_A, var_list=d_A_vars)\n",
    "        self.d_B_trainer = optimizer.minimize(d_loss_B, var_list=d_B_vars)\n",
    "        self.g_A_trainer = optimizer.minimize(g_loss_A, var_list=g_A_vars)\n",
    "        self.g_B_trainer = optimizer.minimize(g_loss_B, var_list=g_B_vars)\n",
    "\n",
    "        # for var in self.model_vars: print(var.name)\n",
    "\n",
    "        #Summary variables for tensorboard\n",
    "        self.g_A_loss_summ = tf.summary.scalar(\"g_A_loss\", g_loss_A)\n",
    "        self.g_B_loss_summ = tf.summary.scalar(\"g_B_loss\", g_loss_B)\n",
    "        self.d_A_loss_summ = tf.summary.scalar(\"d_A_loss\", d_loss_A)\n",
    "        self.d_B_loss_summ = tf.summary.scalar(\"d_B_loss\", d_loss_B)\n",
    "\n",
    "        \n",
    "    def save_training_images(self, sess, epoch):\n",
    "        if not os.path.exists(\"./output/imgs\"):\n",
    "            os.makedirs(\"./output/imgs\")\n",
    "\n",
    "        for i in range(0,10):\n",
    "            fake_A_temp, fake_B_temp, cyc_A_temp, cyc_B_temp = sess.run([self.fake_A, self.fake_B, self.cyc_A, self.cyc_B],\n",
    "                                                                        feed_dict={self.input_A:self.A_input[i], self.input_B:self.B_input[i]})\n",
    "            #*127.5原因：原来的矩阵元素均在[-1,1]区间，+1后移动到[0,2]区间，再*127.5==255/2映射到[0,255]区间，8bit图片的RGB值\n",
    "            imsave(\"./output/imgs/fakeA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8)) #F：B-->A生成的图片\n",
    "            imsave(\"./output/imgs/fakeB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8)) #G：A-->B生成的图片\n",
    "            imsave(\"./output/imgs/cycA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_A_temp[0]+1)*127.5).astype(np.uint8)) #F：G（A）-->A生成的图片\n",
    "            imsave(\"./output/imgs/cycB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((cyc_B_temp[0]+1)*127.5).astype(np.uint8)) #G：F（B）-->B生成的图片\n",
    "            imsave(\"./output/imgs/inputA_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8)) #A的输入图片\n",
    "            imsave(\"./output/imgs/inputB_\"+ str(epoch) + \"_\" + str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8)) #B的输入图片\n",
    "\n",
    "            \n",
    "    def fake_image_pool(self, num_fakes, fake, fake_pool):\n",
    "        ''' This function saves the generated image to corresponding pool of images.\n",
    "        In starting. It keeps on feeling the pool till it is full and then randomly selects an\n",
    "        already stored image and replace it with new one.'''\n",
    "\n",
    "        if(num_fakes < self.pool_size):\n",
    "            fake_pool[num_fakes] = fake\n",
    "            return fake\n",
    "        else :\n",
    "            p = random.random()\n",
    "            if p > 0.5:\n",
    "                random_id = random.randint(0,self.pool_size-1)\n",
    "                temp = fake_pool[random_id]\n",
    "                fake_pool[random_id] = fake\n",
    "                return temp\n",
    "            else :\n",
    "                return fake\n",
    "\n",
    "\n",
    "            \n",
    "    def train(self):\n",
    "        ''' Training Function '''\n",
    "\n",
    "        # Load Dataset from the dataset folder\n",
    "        self.input_setup()\n",
    "\n",
    "        #Build the network\n",
    "        self.model_setup()\n",
    "\n",
    "        #Loss function calculations\n",
    "        self.loss_cal()\n",
    "\n",
    "        # Initializing the global variables;\n",
    "        init = ([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "            #Read input to nd array\n",
    "            self.input_read(sess)\n",
    "\n",
    "            #Restore the model to run the model from last checkpoint\n",
    "            if to_restore:\n",
    "                chkpt_fname = tf.train.latest_checkpoint(self.check_dir)\n",
    "                saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            writer = tf.summary.FileWriter(self.check_dir_write)\n",
    "\n",
    "            if not os.path.exists(self.check_dir_write):\n",
    "                os.makedirs(self.check_dir_write)\n",
    "                \n",
    "            if not os.path.exists(self.check_dir):\n",
    "                os.makedirs(self.check_dir)\n",
    "\n",
    "            # Training Loop\n",
    "            for epoch in range(sess.run(self.global_step),self.max_epoch):\n",
    "                print (\"In the epoch \", epoch)\n",
    "                if save_model == True :\n",
    "                    saver.save(sess,os.path.join(self.check_dir,\"cyclegan\"),global_step=epoch)\n",
    "\n",
    "                # Dealing with the learning rate as per the epoch number\n",
    "                if (epoch < 100) :\n",
    "                    curr_lr = self.learning_rate\n",
    "                else:\n",
    "                    curr_lr = self.learning_rate - self.learning_rate*(epoch-100)/100\n",
    "\n",
    "                if (save_train_image):\n",
    "                    self.save_training_images(sess, epoch)\n",
    "\n",
    "                # sys.exit()\n",
    "\n",
    "                for ptr in range(0,self.max_images):\n",
    "                    print(\"In the iteration \",ptr)\n",
    "                    # print(\"Starting\",time.time()*1000.0)\n",
    "\n",
    "                    # Optimizing the G_A network\n",
    "\n",
    "                    _, fake_B_temp, summary_str = sess.run([self.g_A_trainer, self.fake_B, self.g_A_loss_summ], \n",
    "                                                           feed_dict={self.input_A:self.A_input[ptr], \n",
    "                                                           self.input_B:self.B_input[ptr], \n",
    "                                                           self.lr:curr_lr})\n",
    "\n",
    "                    if save_scalar == True :\n",
    "                        writer.add_summary(summary_str, epoch*self.max_images + ptr)\n",
    "                        \n",
    "                    fake_B_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_B_temp, self.fake_images_B)\n",
    "\n",
    "                    # Optimizing the D_B network\n",
    "                    _, summary_str = sess.run([self.d_B_trainer, self.d_B_loss_summ], \n",
    "                                              feed_dict={self.input_A:self.A_input[ptr], \n",
    "                                              self.input_B:self.B_input[ptr], self.lr:curr_lr, \n",
    "                                              self.fake_pool_B:fake_B_temp1})\n",
    "                    if save_scalar == True :\n",
    "                        writer.add_summary(summary_str, epoch*self.max_images + ptr)\n",
    "\n",
    "\n",
    "                    # Optimizing the G_B network\n",
    "                    _, fake_A_temp, summary_str = sess.run([self.g_B_trainer, self.fake_A, self.g_B_loss_summ],\n",
    "                                                           feed_dict={self.input_A:self.A_input[ptr], \n",
    "                                                           self.input_B:self.B_input[ptr], self.lr:curr_lr})\n",
    "\n",
    "                    if save_scalar == True :\n",
    "                        writer.add_summary(summary_str, epoch*self.max_images + ptr)\n",
    "\n",
    "\n",
    "                    fake_A_temp1 = self.fake_image_pool(self.num_fake_inputs, fake_A_temp, self.fake_images_A)\n",
    "\n",
    "                    # Optimizing the D_A network\n",
    "                    _, summary_str = sess.run([self.d_A_trainer, self.d_A_loss_summ],\n",
    "                                              feed_dict={self.input_A:self.A_input[ptr], \n",
    "                                              self.input_B:self.B_input[ptr], \n",
    "                                              self.lr:curr_lr, self.fake_pool_A:fake_A_temp1})\n",
    "\n",
    "                    if save_scalar == True :\n",
    "                        writer.add_summary(summary_str, epoch*self.max_images + ptr)\n",
    "\n",
    "                    self.num_fake_inputs+=1\n",
    "\n",
    "                sess.run(tf.assign(self.global_step, epoch + 1))\n",
    "\n",
    "            if  save_graph == True :\n",
    "                writer.add_graph(sess.graph)\n",
    "\n",
    "                \n",
    "    def test(self):\n",
    "\n",
    "        ''' Testing Function'''\n",
    "\n",
    "        print(\"Testing the results\")\n",
    "\n",
    "        self.input_setup()\n",
    "\n",
    "        self.model_setup()\n",
    "        saver = tf.train.Saver()\n",
    "        init = tf.global_variables_initializer()\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            sess.run(init)\n",
    "\n",
    "            self.input_read(sess)\n",
    "\n",
    "            chkpt_fname = tf.train.latest_checkpoint(self.check_dir)\n",
    "            saver.restore(sess, chkpt_fname)\n",
    "\n",
    "            if not os.path.exists(\"./output/imgs/test/\"):\n",
    "                os.makedirs(\"./output/imgs/test/\")\n",
    "\n",
    "            for i in range(0,100):\n",
    "                fake_A_temp, fake_B_temp = sess.run([self.fake_A, self.fake_B],\n",
    "                                                    feed_dict={self.input_A:self.A_input[i], \n",
    "                                                    self.input_B:self.B_input[i]})\n",
    "                imsave(\"./output/imgs/test/fakeB_\"+str(i)+\".jpg\",((fake_A_temp[0]+1)*127.5).astype(np.uint8))\n",
    "                imsave(\"./output/imgs/test/fakeA_\"+str(i)+\".jpg\",((fake_B_temp[0]+1)*127.5).astype(np.uint8))\n",
    "                imsave(\"./output/imgs/test/inputA_\"+str(i)+\".jpg\",((self.A_input[i][0]+1)*127.5).astype(np.uint8))\n",
    "                imsave(\"./output/imgs/test/inputB_\"+str(i)+\".jpg\",((self.B_input[i][0]+1)*127.5).astype(np.uint8))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = CycleGAN()\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
